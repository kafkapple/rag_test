"""
app.py

Streamlit + Hydra + dotenv
ëª¨ë“  ëª¨ë“ˆ ì¡°ë¦½í•˜ì—¬ ìµœì¢… RAG QA.
Ensemble Retriever ì‹œì—°.
Semantic Splitter ì‹œì—°.
"""
import os
import streamlit as st
import hydra
from omegaconf import DictConfig
from dotenv import load_dotenv
from hydra.core.global_hydra import GlobalHydra
from hydra import initialize, compose
import time
import traceback

from src.ingestion.loaders import (
    load_pdf_document, load_txt_document, load_web_document
)
from src.ingestion.splitter import split_documents
from src.embeddings.embedding_factory import get_embedding_model
from src.db import VectorStoreWrapper
from src.retriever.retriever_factory import get_retriever

from src.pipeline.prompt_template import PromptTemplateManager
from src.pipeline.rag_chain import RAGPipeline

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError(
        "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
        ".env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
    )
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["HF_HOME"] = "./models/"
# Streamlit ì„¤ì •
st.set_page_config(
    page_title="RAG Pipeline Demo",
    layout="wide",
    initial_sidebar_state="expanded"
)

# íŒŒì¼ ê°ì‹œ ë¹„í™œì„±í™”
if not st.session_state.get("watched_files"):
    st.session_state["watched_files"] = True
    import streamlit.watcher.path_watcher
    streamlit.watcher.path_watcher.watch_file = lambda x: None

def display_split_documents():
    """ë¶„í• ëœ ë¬¸ì„œë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    if "docs_by_source" not in st.session_state:
        return
        
    st.write("### ë¶„í• ëœ ë¬¸ì„œ ì¡°íšŒ")
    docs_by_source = st.session_state["docs_by_source"]
    
    for source, source_docs in docs_by_source.items():
        st.subheader(f"ğŸ“„ {source} ({len(source_docs)} chunks)")
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        chunks_per_page = 5
        total_pages = len(source_docs) // chunks_per_page + (1 if len(source_docs) % chunks_per_page else 0)
        
        # sourceì—ì„œ ìœ íš¨í•œ key ë¬¸ìì—´ ìƒì„±
        safe_key = "".join(c if c.isalnum() else "_" for c in source)
        
        col1, col2 = st.columns([1, 4])
        with col1:
            page = st.number_input(
                "Page",
                min_value=1,
                max_value=total_pages,
                value=1,
                key=f"page_{safe_key}"
            )
        with col2:
            st.write(f"Total: {total_pages} pages")
        
        start_idx = (page - 1) * chunks_per_page
        end_idx = min(start_idx + chunks_per_page, len(source_docs))
        
        # íƒ­ìœ¼ë¡œ ì²­í¬ í‘œì‹œ
        chunk_tabs = st.tabs([f"Chunk {i+1}" for i in range(start_idx, end_idx)])
        for tab, i in zip(chunk_tabs, range(start_idx, end_idx)):
            with tab:
                doc = source_docs[i]
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**ë‚´ìš©:**")
                    st.markdown(doc.page_content)
                with col2:
                    st.markdown("**ë©”íƒ€ë°ì´í„°:**")
                    st.json(doc.metadata)
        
        st.markdown("---")  # ì†ŒìŠ¤ êµ¬ë¶„ì„ 

def main():
    # Hydra ì¬ì´ˆê¸°í™”
    GlobalHydra.instance().clear()
    initialize(config_path="configs", version_base=None)
    cfg = compose(config_name="config")
    
    load_dotenv()
    st.title(cfg.app.project_name)

    # ----------------------------
    # 1) Load Documents
    # ----------------------------
    st.header("1) Load Documents")
    
    # ì´ˆê¸°í™”
    if "pdf_docs" not in st.session_state:
        st.session_state.pdf_docs = []
    if "web_docs" not in st.session_state:
        st.session_state.web_docs = []
    
    # ì˜ˆì‹œ: PDF
    pdf_path = st.text_input("PDF path:", cfg.paths.pdf_path)
    if st.button("Load PDF"):
        st.session_state.pdf_docs = load_pdf_document(pdf_path)
        st.write(f"Loaded {len(st.session_state.pdf_docs)} pages from PDF.")
    
    # ì˜ˆì‹œ: ì›¹
    web_url = st.text_input("Web URL:", cfg.crawler.start_url)
    if st.button("Load Web"):
        st.session_state.web_docs = load_web_document(web_url)
        st.write(f"Loaded {len(st.session_state.web_docs)} docs from Web page.")

    # ì „ì²´ ë¬¸ì„œ í•©ì¹˜ê¸°
    docs = st.session_state.pdf_docs + st.session_state.web_docs
    st.write(f"Total raw docs: {len(docs)}")

    # ----------------------------
    # 2) Split Documents
    # ----------------------------
    st.header("2) Split Documents")
    
    # Splitter ì˜µì…˜ ì„¤ì •
    st.subheader("Splitter ì„¤ì •")
    col1, col2, col3 = st.columns(3)
    with col1:
        splitter_type = st.selectbox(
            "Splitter íƒ€ì…",
            options=["character", "token", "markdown", "recursive"],
            help="ë¬¸ì„œ ë¶„í•  ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”"
        )
    with col2:
        chunk_size = st.number_input(
            "Chunk Size",
            min_value=100,
            max_value=2000,
            value=cfg.splitter.chunk_size,
            help="ê° ì²­í¬ì˜ ìµœëŒ€ ê¸¸ì´"
        )
    with col3:
        chunk_overlap = st.number_input(
            "Chunk Overlap",
            min_value=0,
            max_value=chunk_size-1,
            value=cfg.splitter.chunk_overlap,
            help="ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ê¸¸ì´"
        )
    
    if st.button("Split Docs"):
        if not docs:
            st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
            st.write("### ë””ë²„ê·¸ ì •ë³´:")
            st.write(f"PDF ë¬¸ì„œ ìˆ˜: {len(st.session_state.pdf_docs)}")
            st.write(f"ì›¹ ë¬¸ì„œ ìˆ˜: {len(st.session_state.web_docs)}")
            return
            
        try:
            st.write("### ë¬¸ì„œ ë¶„í•  ì‹œì‘")
            st.write(f"ì…ë ¥ ë¬¸ì„œ ìˆ˜: {len(docs)}")
            
            splitted_docs = split_documents(
                docs,
                splitter_type=splitter_type,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            if not splitted_docs:
                st.error("ë¬¸ì„œ ë¶„í•  ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return
                
            st.write(f"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(splitted_docs)}")
            
            # ë¬¸ì„œë¥¼ ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
            docs_by_source = {}
            for doc in splitted_docs:
                source = doc.metadata.get('source', 'unknown')
                if source not in docs_by_source:
                    docs_by_source[source] = []
                docs_by_source[source].append(doc)
            
            # session_stateì— ì €ì¥
            st.session_state["splitted_docs"] = splitted_docs
            st.session_state["docs_by_source"] = docs_by_source
            st.success("ë¬¸ì„œ ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.write("### ë””ë²„ê·¸ ì •ë³´:")
            st.write(f"ë¬¸ì„œ íƒ€ì…: {type(docs)}")
            st.write(f"ì„¤ì • ì •ë³´:")
            st.write(f"- splitter_type: {splitter_type}")
            st.write(f"- chunk_size: {chunk_size}")
            st.write(f"- chunk_overlap: {chunk_overlap}")
            st.code(traceback.format_exc())

    # ë¬¸ì„œê°€ ë¶„í• ë˜ì–´ ìˆì„ ë•Œë§Œ í•œ ë²ˆ í‘œì‹œ
    if "docs_by_source" in st.session_state and not st.session_state.get("display_triggered"):
        display_split_documents()
        st.session_state["display_triggered"] = True

    # ----------------------------
    # 3) Embeddings & VectorStore
    # ----------------------------
    st.header("3) Create Vector Store")
    
    # ì„ë² ë”© ì„¤ì •
    st.subheader("ì„ë² ë”© ì„¤ì •")
    emb_providers = ["huggingface", "openai"]
    emb_provider = st.selectbox(
        "ì„ë² ë”© ì œê³µì",
        options=emb_providers,
        index=emb_providers.index(cfg.embeddings.provider)
    )
    
    if emb_provider == "huggingface":
        hf_models = [
            "sentence-transformers/all-MiniLM-L6-v2",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "jhgan/ko-sroberta-multitask",
        ]
        emb_model_id = st.selectbox(
            "Hugging Face ì„ë² ë”© ëª¨ë¸",
            options=hf_models,
            index=hf_models.index(cfg.embeddings.model_configs.huggingface.model_id)
        )
    else:
        openai_models = ["text-embedding-ada-002"]
        emb_model_id = st.selectbox(
            "OpenAI ì„ë² ë”© ëª¨ë¸",
            options=openai_models,
            index=openai_models.index(cfg.embeddings.model_configs.openai.model_id)
        )

    if "splitted_docs" not in st.session_state:
        st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € ë¶„í• í•´ì£¼ì„¸ìš”.")
    else:
        splitted_docs = st.session_state["splitted_docs"]
        if st.button("Create VectorStore"):
            try:
                st.write("### ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹œì‘")
                # init embeddings
                st.write(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (provider: {emb_provider}, model: {emb_model_id})")
                
                # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
                if emb_provider == "openai" and not os.getenv("OPENAI_API_KEY"):
                    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return
                if emb_provider == "huggingface" and not os.getenv("HUGGINGFACE_TOKEN"):
                    st.error("Hugging Face í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return
                
                emb_model = get_embedding_model(
                    provider=emb_provider,
                    model_id=emb_model_id
                )
                
                # ì„ë² ë”© í…ŒìŠ¤íŠ¸
                try:
                    st.write("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘...")
                    test_text = splitted_docs[0].page_content
                    st.write(f"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {test_text[:100]}...")
                    test_embedding = emb_model.embed_query(test_text)
                    st.write(f"ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
                except Exception as e:
                    st.error(f"ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.write("### ë””ë²„ê·¸ ì •ë³´:")
                    st.write(f"- HuggingFace í† í°: {'ì„¤ì •ë¨' if os.getenv('HUGGINGFACE_TOKEN') else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
                    st.write(f"- ëª¨ë¸: {emb_model_id}")
                    st.code(traceback.format_exc())
                    return
                
                if not test_embedding:
                    st.error("ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    return
                
                st.write("ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
                vstore = VectorStoreWrapper(
                    vtype=cfg.vectorstore.type,
                    persist_dir=cfg.vectorstore.persist_dir,
                    embeddings=emb_model
                )
                
                st.write(f"ë¬¸ì„œ {len(splitted_docs)}ê°œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€í•˜ëŠ” ì¤‘...")
                vstore.create_from_documents(splitted_docs)
                st.session_state["vector_store"] = vstore
                st.success("ë²¡í„° ì €ì¥ì†Œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.write("### ë””ë²„ê·¸ ì •ë³´:")
                st.write(f"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(splitted_docs)}")
                st.write(f"ì„¤ì • ì •ë³´:")
                st.write(f"- vectorstore type: {cfg.vectorstore.type}")
                st.write(f"- persist_dir: {cfg.vectorstore.persist_dir}")
                st.write(f"- embedding provider: {emb_provider}")
                st.write(f"- embedding model: {emb_model_id}")
                st.code(traceback.format_exc())

    # ----------------------------
    # 4) LLM ì„¤ì •
    # ----------------------------
    st.header("4) LLM ì„¤ì •")
    llm_providers = ["huggingface", "openai"]
    llm_provider = st.selectbox(
        "LLM ì œê³µì",
        options=llm_providers,
        index=llm_providers.index(cfg.llm.provider)
    )
    
    if llm_provider == "huggingface":
        hf_llm_models = [
            "Qwen/Qwen1.5-0.5B-Chat",  # 0.5B ëª¨ë¸
            "Qwen/Qwen2.5-7B-Instruct",  # 1.8B ëª¨ë¸
        ]
        llm_model_id = st.selectbox(
            "Hugging Face ëª¨ë¸",
            options=hf_llm_models,
            index=hf_llm_models.index(cfg.llm.model_configs.huggingface.model_id)
        )
    else:
        openai_llm_models = ["gpt-3.5-turbo", "gpt-4"]
        llm_model_id = st.selectbox(
            "OpenAI ëª¨ë¸",
            options=openai_llm_models,
            index=openai_llm_models.index(cfg.llm.model_configs.openai.model_id)
        )
    
    temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=cfg.llm.model_configs[llm_provider].temperature,
        step=0.1
    )

    # ----------------------------
    # 5) Retriever
    # ----------------------------
    st.header("5) Create Retriever")
    
    # Retriever ì„¤ì •
    st.subheader("Retriever ì„¤ì •")
    retriever_types = ["bm25", "faiss", "ensemble"]
    retriever_type = st.selectbox(
        "Retriever íƒ€ì…",
        options=retriever_types,
        index=retriever_types.index(cfg.retriever.type),
        help="ë¬¸ì„œ ê²€ìƒ‰ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    top_k = st.number_input(
        "ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (Top K)",
        min_value=1,
        max_value=20,
        value=cfg.retriever.top_k,
        help="ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œì˜ ìˆ˜"
    )

    if st.button("Setup Retriever"):
        if "vector_store" not in st.session_state:
            st.warning("Create vector store first.")
        else:
            try:
                vstore = st.session_state["vector_store"]
                # for BM25 we need raw texts
                raw_texts = [doc.page_content for doc in st.session_state["splitted_docs"]]
                
                retriever = get_retriever(
                    rtype=retriever_type,
                    doc_texts=raw_texts,
                    vector_store=vstore,
                    top_k=top_k
                )
                
                st.session_state["retriever"] = retriever
                st.success(f"Retriever ({retriever_type}) ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                st.error(f"Retriever ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.code(traceback.format_exc())

    # ----------------------------
    # 6) RAG Pipeline
    # ----------------------------
    st.header("6) RAG QA")
    
    # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì„ íƒ
    st.subheader("í”„ë¡¬í”„íŠ¸ ì„¤ì •")
    prompt_mgr = PromptTemplateManager()
    prompt_styles = list(prompt_mgr.templates.keys())
    
    col1, col2 = st.columns([1, 2])
    with col1:
        selected_style = st.selectbox(
            "í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼",
            options=prompt_styles,
            index=prompt_styles.index("concise") if "concise" in prompt_styles else 0,
            help="ë‹µë³€ ìƒì„± ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì„¸ìš”"
        )
    
    # ì„ íƒëœ í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸°
    with col2:
        st.text_area(
            "í…œí”Œë¦¿ ë¯¸ë¦¬ë³´ê¸°",
            value=prompt_mgr.templates[selected_style],
            height=150,
            disabled=True
        )
    
    # ì§ˆë¬¸ ì…ë ¥
    query = st.text_input("Your question:")
    if st.button("Get Answer"):
        if "retriever" not in st.session_state:
            st.warning("Please setup retriever first.")
            return
            
        ret = st.session_state["retriever"]
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¨¼ì € í‘œì‹œ
        st.write("### Retrieved Documents")
        relevant_docs = ret.get_relevant_documents(query)
        for i, doc in enumerate(relevant_docs, 1):
            with st.expander(f"Document {i}"):
                st.write(doc.page_content)
                st.write(f"Source: {doc.metadata.get('source', 'unknown')}")
        
        # ëª¨ë¸ ì´ˆê¸°í™” ë° ë‹µë³€ ìƒì„±
        progress_text = st.empty()
        progress_bar = st.progress(0)
        
        def progress_callback(step: str, progress: float):
            progress_bar.progress(progress)
            progress_text.write(f"ğŸ¤– {step}")
        
        with st.spinner("Generating answer..."):
            try:
                rag = RAGPipeline(
                    retriever=ret,
                    prompt_manager=prompt_mgr,
                    llm_name=cfg.llm.model_configs[llm_provider].model_id,
                    temperature=temperature,
                    progress_callback=progress_callback
                )
                
                answer = rag.run(query, style=selected_style)
                
                # ì§„í–‰ ìƒíƒœ í‘œì‹œ ì œê±°
                progress_text.empty()
                progress_bar.empty()
                
                st.write("### Answer")
                st.markdown(answer)
                
                # ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
                with st.expander("Debug Info"):
                    st.text("Check the terminal/console for detailed debug output")
                
            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")
                st.code(traceback.format_exc())

    # ----------------------------
    # 7) Document Search
    # ----------------------------
    st.header("7) Search Documents")
    search_query = st.text_input("Search query:")
    k = st.slider("Number of documents", 1, 10, 3)
    
    if st.button("Search"):
        if "vector_store" not in st.session_state:
            st.warning("Please create vector store first.")
        else:
            vstore = st.session_state["vector_store"]
            docs = vstore.similarity_search(search_query, k=k)
            
            st.write("### Search Results")
            for i, doc in enumerate(docs, 1):
                st.write(f"**Document {i}**")
                st.write(doc.page_content)
                st.write("---")

if __name__ == "__main__":
    main()
